import os
import socket
import signal
import time
import threading
import Queue
import random
import select
import logging
import netaddr

from reactor.config import Config
from reactor.loadbalancer.connection import LoadBalancerConnection
import reactor.loadbalancer.netstat as netstat

def close_fds(except_fds=None):
    if except_fds is None:
        except_fds = []

    try:
        maxfd = os.sysconf("SC_OPEN_MAX")
    except (AttributeError, ValueError):
        maxfd = 1024
    for fd in range(0, maxfd):
        if not(fd in except_fds):
            try:
                os.close(fd)
            except OSError:
                pass

def fork_and_exec(cmd, child_fds=None):
    if child_fds is None:
        child_fds = []

    # Close all file descriptors, except
    # for those we have been specified to keep.
    # These file descriptors are closed in the
    # parent post fork.
    # Create a pipe to communicate grandchild PID.
    (r, w) = os.pipe()

    # Fork
    pid = os.fork()
    if pid != 0:
        # Close writing end of the pipe.
        os.close(w)

        # Wait for the child to exit.
        while True:
            (rpid, status) = os.waitpid(pid, 0)
            if rpid == pid:
                if os.WEXITSTATUS(status) > 0:
                    # Something went wrong, clean up.
                    os.close(r)
                    return None
                else:
                    # Get a file object for the read end.
                    r_obj = os.fdopen(r, "r")

                    # Read grandchild PID from pipe and close it.
                    try:
                        child = int(r_obj.readline())
                        return child
                    except ValueError:
                        return None
                    finally:
                        r_obj.close()

    # Close off all parent FDs.
    close_fds(except_fds=child_fds + [w])

    # Create process group.
    os.setsid()

    # Fork again.
    pid = os.fork()
    if pid != 0:
        # Write grandchild pid to pipe
        w_obj = os.fdopen(w, "w")
        w_obj.write(str(pid) + "\n")
        w_obj.flush()

        # Exit (hard).
        os._exit(0)

    # Close the write end of the pipe
    os.close(w)

    # Exec the given command.
    os.execvp(cmd[0], cmd)

def _as_client(src_ip, src_port):
    return "%s:%d" % (src_ip, src_port)

class Accept(object):

    def __init__(self, sock):
        (client, address) = sock.accept()
        # Ensure that the underlying socket is closed.
        # It's probably crazy pills -- but I saw weird
        # issues with the socket object. This way we only
        # keep around the file descriptor nothing more.
        self.fd = os.dup(client.fileno())
        client._sock.close()
        self.src = address
        self.dst = sock.getsockname()

    def __del__(self):
        # If the Accept object hasn't been cleaned up,
        # we ensure that it drops on garbage collection.
        self.drop()

    def drop(self):
        if self.fd is not None:
            try:
                os.close(self.fd)
            except IOError:
                logging.error("Dropping connection from %s: bad FD %d",
                    _as_client(*(self.src)), self.fd)
            self.fd = None

    def redirect(self, host, port):
        if self.fd is not None:
            cmd = [
                "socat",
                "fd:%d" % self.fd,
                "tcp-connect:%s:%d" % (host, port)
            ]
            child = fork_and_exec(cmd, child_fds=[self.fd])
            if child:
                os.close(self.fd)
                self.fd = None
            return child

class ConnectionConsumer(threading.Thread):

    def __init__(self, locks, producer):
        super(ConnectionConsumer, self).__init__()
        self.daemon = True
        self.execute = True

        self.locks = locks
        self.producer = producer

        self.portmap = {}
        self.postponed = []
        self.standby = {}
        self.children = {}
        self.cond = threading.Condition()

        # Subscribe to events generated by the producer.
        # NOTE: These are cleaned up in stop().
        self.producer.subscribe(self.notify)

        # Start the thread.
        super(ConnectionConsumer, self).start()

    def set(self, portmap):
        self.cond.acquire()
        self.portmap = portmap
        self.cond.notify()
        self.cond.release()

    def stop(self):
        self.cond.acquire()
        self.execute = False
        self.cond.notify()
        self.cond.release()
        self.join()

        # Unsubscribe from notifications.
        self.producer.unsubscribe(self.notify)

        # Clean up leftover children.
        self.kill_children()
        self.reap_children()
        self.clear_standby(force=True)

    def notify(self):
        self.cond.acquire()
        self.cond.notify()
        self.cond.release()

    def handle(self, connection):
        self.cond.acquire()
        try:
            # Index by the destination port.
            port = connection.dst[1]
            if not(self.portmap.has_key(port)):
                connection.drop()
                return True

            # Create a map of the IPs.
            (_, exclusive, reconnect, backends, client_subnets) = self.portmap[port]
            ipmap = {}
            ipmap.update(backends)
            ips = ipmap.keys()

            # Check the subnet.
            if client_subnets:
                subnet_okay = False
                for subnet in client_subnets:
                    if netaddr.ip.IPAddress(str(connection.src[0])) in \
                       netaddr.ip.IPNetwork(str(subnet)):
                        subnet_okay = True
                        break
                if not subnet_okay:
                    connection.drop()
                    return True

            # Find a backend IP (exclusive or not).
            ip = None
            if exclusive:
                # See if we have a VM to reconnect to.
                if reconnect > 0:
                    existing = self.locks.find(connection.src[0])
                    if len(existing) > 0:
                        ip = existing[0]
                        if ip in self.standby:
                            # NOTE: We will have a lock representing
                            # this connection, but is it already held.
                            del self.standby[ip]
                if not ip:
                    ip = self.locks.lock(ips, value=connection.src[0])
            else:
                ip = ips[random.randint(0, len(ips)-1)]

            if ip:
                # Either redirect or drop the connection.
                child = connection.redirect(ip, ipmap[ip])
                standby_time = (exclusive and reconnect)
                if child:
                    self.children[child] = (ip, connection, standby_time)
                    return True
            return False
        finally:
            self.cond.release()

    def clear_standby(self, force=False):
        removed = []
        now = time.time()
        for (ip, timeout) in self.standby.items():
            if force or timeout < now:
                self.locks.remove(ip)
                removed.append(ip)
        for ip in removed:
            del self.standby[ip]
        return len(removed)

    def run(self):
        self.cond.acquire()
        while self.execute:
            connection = self.producer.next()

            # Service connection, if any.
            if connection:
                if not self.handle(connection):
                    # now, we add it to the list of postponed
                    # connections, which we will try again shortly.
                    self.postponed.append(connection)

                # Continue servicing connections while
                # there is an active queue in the producer.
                continue

            # Try servicing postponed connections.
            new_postponed = []
            for connection in self.postponed:
                if not self.handle(connection):
                    # Arg, still not handled. We keep the connection
                    # on our list of postponed connections and continue.
                    new_postponed.append(connection)
            self.postponed = new_postponed

            # Clear any standby IPs.
            if self.clear_standby():
                continue

            # Reap children.
            if self.reap_children():
                continue

            # Wait for a second.
            # This will be waken by either producer events,
            # or by more backends appearing that may make
            # available new backends for us to schedule.
            # The only event that won't wake this is an
            # existing connection expiring.
            self.cond.wait(1.0)
        self.cond.release()

    def kill_children(self):
        # Kill all children.
        for child in self.children.keys():
            try:
                os.kill(child, signal.SIGTERM)
            except OSError:
                # Process no longer exists.
                pass

    def reap_children(self):
        reaped = 0

        # Reap dead children.
        for child in self.children.keys():
            try:
                # Check if child is alive.
                os.kill(child, 0)
            except OSError:
                # Not alive - remove from children list.
                (ip, conn, standby_time) = self.children[child]
                del self.children[child]
                reaped += 1

                # If reconnect and exclusive is on, then
                # we add this connection to the standby list.
                # NOTE: At this point, you only get on the standby
                # list if the IP is exclusive and with reconnect.
                # This means that it will *not* get selected again
                # and the only necessary means of removing the IP
                # is through the clear_standby() hook.
                if standby_time:
                    self.standby[ip] = time.time() + standby_time
                else:
                    self.locks.remove(ip)

        # Return the number of children reaped.
        # This means that callers can do if self.reap_children().
        return reaped

    def sessions(self):
        return self.consumer.sessions()

    def drop_session(self, client, backend):
        self.consumer.drop_session(client, backend)

    def pending(self):
        pending = {}
        self.cond.acquire()
        for connection in self.postponed:
            # Index by the destination port.
            port = connection.dst[1]
            if not(self.portmap.has_key(port)):
                continue

            # Get the associated URL for the postponed connection.
            (url, _, _, _, _) = self.portmap[port]
            if not pending.has_key(url):
                pending[url] = 1
            else:
                pending[url] += 1
        self.cond.release()
        return pending

    def metrics(self):
        metric_map = {}
        self.cond.acquire()
        ips = [ip for (ip, _, _) in self.children.values()]
        ips.extend(self.standby.keys())
        for ip in ips:
            if not ip in metric_map:
                # Set the current active count to one.
                metric_map[ip] = { "active" : (1, 1) }
            else:
                # Add one to our current active count.
                cur_count = metric_map[ip]["active"][1]
                metric_map[ip]["active"] = (1, cur_count+1)
        self.cond.release()
        return metric_map

    def sessions(self):
        session_map = {}
        self.cond.acquire()
        for (ip, conn, _) in self.children.values():
            (src_ip, src_port) = conn.src
            # We store clients as ip:port pairs.
            # This matters for below, where we must match
            # in order to drop the session.
            ip_sessions = session_map.get(ip, [])
            ip_sessions.append(_as_client(src_ip, src_port))
            session_map[ip] = ip_sessions
        self.cond.release()
        return session_map

    def drop_session(self, client, backend):
        self.cond.acquire()
        for child in self.children.keys():
            (ip, conn, _) = self.children[child]
            (src_ip, src_port) = conn.src
            if client == _as_client(src_ip, src_port) and backend == ip:
                try:
                    os.kill(child, signal.SIGTERM)
                except OSError:
                    # The process no longer exists.
                    pass
        self.cond.release()

class ConnectionProducer(threading.Thread):

    # On cleanup --
    # When this class is garbage collected, all
    # the pending connections will be dropped.
    # This is due to the fact that everything in
    # the queue is wrapped in the Accept class
    # above, and when these objects are deleted,
    # they will explicitly drop the connection.

    def __init__(self):
        super(ConnectionProducer, self).__init__()
        self.daemon = True
        self.execute = True

        self.epoll = None
        self.queue = Queue.Queue()
        self.sockets = {}
        self.filemap = {}
        self.cond = threading.Condition()
        self.notifiers = []
        self.set()

        self._update_epoll()

        # Start the thread.
        super(ConnectionProducer, self).start()

    def stop(self):
        self.execute = False
        if hasattr(self, 'epoll'):
            self.epoll.close()
        self.join()

    def set(self, ports=None):
        if ports is None:
            ports = []

        # Set the appropriate ports.
        self.cond.acquire()
        try:
            for port in ports:
                if not(self.sockets.has_key(port)):
                    sock = socket.socket()
                    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                    try:
                        sock.bind(("", port))
                    except IOError as ioe:
                        # Can't bind this port (likely already in use), so skip it.
                        logging.warning("Can't bind port %d: %s", port, ioe.strerror)
                        continue
                    sock.listen(10)
                    self.sockets[port] = sock
                    self.filemap[sock.fileno()] = sock

            ports_to_delete = []
            for port in self.sockets:
                if not(port in ports):
                    sock = self.sockets[port]
                    del self.filemap[sock.fileno()]
                    sock.close()
                    ports_to_delete.append(port)

            # Clean old ports out after iterating.
            for port in ports_to_delete:
                del self.sockets[port]

            self._update_epoll()
        finally:
            self.cond.release()

    def _update_epoll(self):
        self.epoll = select.epoll()
        for sock in self.sockets.values():
            self.epoll.register(sock.fileno(), select.EPOLLIN)

    def subscribe(self, cb):
        self.cond.acquire()
        self.notifiers.append(cb)
        self.cond.release()

    def unsubscribe(self, cb):
        self.cond.acquire()
        self.notifiers.remove(cb)
        self.cond.release()

    def next(self):
        try:
            # Pull the next connection.
            return self.queue.get(block=False)
        except Queue.Empty:
            return None

    def run(self):
        while self.execute:
            try:
                # Poll for events.
                events = self.epoll.poll(1)
            except IOError:
                # Whoops -- could just be an interrupted system call.
                # (To be specific, I see this error when attaching via
                # strace to the process). We just let it go again...
                continue

            # Scan the events and accept.
            self.cond.acquire()
            for fileno, event in events:
                if not(fileno in self.filemap):
                    # Stale connection. Update epoll.
                    self._update_epoll()
                    continue

                # Check that it's a read event.
                assert (event & select.EPOLLIN) == select.EPOLLIN

                # Create a new connection object.
                sock = self.filemap[fileno]
                connection = Accept(sock)

                # Notify all listens that there
                # are new connections available.
                self.queue.put(connection)
                for notifier in self.notifiers:
                    notifier()
            self.cond.release()

class TcpEndpointConfig(Config):

    exclusive = Config.boolean(label="One VM per connection", default=True,
        description="Each Instance is used exclusively to serve a single connection.")

    reconnect = Config.integer(label="Reconnect Timeout", default=60,
        description="Amount of time a disconnected client has to reconnect before" \
                    + " the VM is returned to the pool.")

    client_subnets = Config.list(label="Client Subnets", order=7,
        description="Only allow connections from these client subnets.")

class Connection(LoadBalancerConnection):
    """ Raw TCP """

    _ENDPOINT_CONFIG_CLASS = TcpEndpointConfig
    _SUPPORTED_URLS = {
        "tcp://([1-9][0-9]*)": lambda m: int(m.group(1))
    }

    producer = None
    consumer = None

    def __init__(self, **kwargs):
        super(Connection, self).__init__(**kwargs)

        self.portmap = {}
        self.active = set()

        self.producer = ConnectionProducer()
        self.consumer = ConnectionConsumer(self.locks, self.producer)

    def __del__(self):
        self.producer.set([])
        self.consumer.set([])
        self.producer.stop()
        self.consumer.stop()
        if self.locks:
            self.locks.clear()

    def change(self, url, backends, config=None):
        # Grab the listen port.
        listen = self.url_info(url)

        # Ensure that we can't end up in a loop.
        looping_ips = [backend.ip for backend in backends
            if (backend.port == listen or backend.port == 0)
                and backend.ip.startswith("127.")]

        if len(looping_ips) > 0:
            logging.error("Attempted TCP loop.")
            return

        # Clear existing data.
        if self.portmap.has_key(listen):
            del self.portmap[listen]

        # If no backends, don't queue anything.
        if len(backends) == 0:
            return

        # Build our list of backends.
        config = self._endpoint_config(config)
        portmap_backends = []
        for backend in backends:
            if not(backend.port):
                port = listen
            else:
                port = backend.port
            portmap_backends.append((backend.ip, port))
        # Update the portmap (including exclusive info).
        # NOTE: The reconnect/exclusive/client_subnets parameters
        # control how backends are mapped by the consumer, how machines
        # are cleaned up, etc. See the Consumer class and the metrics()
        # function below to understand the interactions there.
        self.portmap[listen] = (
            url,
            config.exclusive,
            config.reconnect,
            portmap_backends,
            config.client_subnets)

    def save(self):
        self.consumer.set(self.portmap)
        self.producer.set(self.portmap.keys())

    def metrics(self):
        return self.consumer.metrics()

    def sessions(self):
        return self.consumer.sessions()

    def drop_session(self, client, backend):
        self.consumer.drop_session(client, backend)

    def pending(self):
        return self.consumer.pending()
